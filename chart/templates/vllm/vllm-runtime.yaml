---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
labels:
  opendatahub.io/dashboard: 'true'
metadata:
  annotations:
    openshift.io/display-name: 'CUSTOM-vLLM'
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/apiProtocol: REST
  name: vllm
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
    - args:
        - --port=8080
        - --model=/mnt/models
        - --served-model-name={{.Name}}
        - --dtype
        - float16
        - --max-lora-rank
        - "64"
        - --enable-lora
        - --lora-modules
        - dcot=/mnt/models/lora/phi-2-dcot/
        - doctor=/mnt/models/lora/phi2-doctor28e/
        - --chat-template
        - /prompt/chat.jinja
        - --uvicorn-log-level
        - debug
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      image: quay.io/modh/vllm@sha256:4f550996130e7d16cacb24ca9a2865e7cf51eddaab014ceaf31a1ea6ef86d4ec
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
        - mountPath: /prompt/chat.jinja
          subPath: chat.jinja
          name: chat-template
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
    - name: chat-template
      configMap:
        name: chat-template
        items:
          - key: "chat.jinja"
            path: "chat.jinja"
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM

